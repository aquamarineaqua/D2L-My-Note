{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "torch.set_printoptions(precision=8)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的合并 —— 拼接、堆叠（tensorflow）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 35, 8)\n",
      "(2, 35, 8)\n",
      "(6, 35, 8)\n",
      "--------------------\n",
      "(2, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "# 张量的合并 —— 拼接  tf.concat\n",
    "a = tf.random.normal([4, 35, 8])  # 比如该数据为：4个班级，35个学生，8门课的成绩\n",
    "print(a.shape)\n",
    "\n",
    "b = tf.random.normal([2, 35, 8])  # 比如该数据为另一个年级的数据\n",
    "print(b.shape)\n",
    "\n",
    "# 在第一维拼接：把所有班级拼起来\n",
    "c = tf.concat([a, b], axis=0)\n",
    "print(c.shape)\n",
    "print('-'*20)\n",
    "\n",
    "# 张量的合并 —— 堆叠  tf.stack\n",
    "# 堆叠即创建一个新的维度，在新的维度上合并数据。待堆叠数据的shape必须相同。\n",
    "a = tf.random.normal([28,28,3])\n",
    "b = tf.random.normal([28,28,3])\n",
    "c = tf.stack([a, b], axis=0)   # 把两张图片堆叠起来，形成图像集\n",
    "print(c.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的合并 —— 拼接、堆叠（pytorch）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 35, 8])\n",
      "torch.Size([2, 35, 8])\n",
      "torch.Size([6, 35, 8])\n",
      "--------------------\n",
      "torch.Size([2, 28, 28, 3])\n"
     ]
    }
   ],
   "source": [
    "# 张量的合并 —— 拼接    torch.cat\n",
    "a2 = torch.randn(4, 35, 8)  # 比如该数据为：4个班级，35个学生，8门课的成绩\n",
    "print(a2.shape)\n",
    "\n",
    "b2 = torch.randn(2, 35, 8)  # 比如该数据为另一个年级的数据\n",
    "print(b2.shape)\n",
    "\n",
    "# 在第一维拼接：把所有班级拼起来\n",
    "c2 = torch.cat([a2, b2], dim=0)\n",
    "print(c2.shape)\n",
    "print('-'*20)\n",
    "\n",
    "# 张量的合并 —— 堆叠    torch.stack\n",
    "# 堆叠即创建一个新的维度，在新的维度上合并数据。待堆叠数据的shape必须相同。\n",
    "a2 = torch.randn(28,28,3)\n",
    "b2 = torch.randn(28,28,3)\n",
    "c2 = torch.stack([a2, b2], dim=0)   # 把两张图片堆叠起来，形成图像集\n",
    "print(c2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的分割（tensorflow）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(2, 28, 28, 3)\n",
      "--------------------\n",
      "4\n",
      "(0, 28, 28, 3)\n",
      "tf.Tensor([], shape=(0, 28, 28, 3), dtype=float32)\n",
      "(28, 28, 3)\n",
      "--------------------\n",
      "(4, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 张量的分割    tf.split、tf.unstack\n",
    "\n",
    "# 张量的分割即将张量拆分为多个张量，需要指定分割方案（num_or_size_splits，传入num即指定分割为多少份，传入一个List即指定按列表制定的方案分割）\n",
    "\n",
    "a = tf.random.normal([4, 28, 28, 3])  # a是一个图像集，有4张图\n",
    "result = tf.split(a, 2, axis=0)  # 指定分割为两份\n",
    "print(type(result))  # 返回一个列表，列表元素为两个tensor\n",
    "print(result[0].shape)  # 第一份是一个2张图片的图像集\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "a = tf.random.normal([4, 28, 28, 3])  # a是一个图像集，有4张图\n",
    "result = tf.split(a, [1,2,1,0], axis=0)  # 指定分割方式，传入一个List\n",
    "print(len(result))  # 被分割成了4份\n",
    "print(result[-1].shape)  # 最后一份没有图片\n",
    "print(result[-1])  # 是一个空tensor，只有形状\n",
    "print(result[0][0].shape)  # 可访问第一份的那张图片\n",
    "\n",
    "print('-'*20)\n",
    "# 如果想等份分割，每份都只有1个元素，可用 unstack 方法。分完后，分割的那个维度消失\n",
    "a = tf.random.normal([4, 28, 28, 3])\n",
    "result = tf.unstack(a, axis=3)  # 把最后一个维度，即通道进行分割\n",
    "print(result[0].shape)  # 分完后，被分割的那个维度消失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的分割（pytorch）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "4\n",
      "torch.Size([1, 28, 28, 3])\n",
      "--------------------\n",
      "4\n",
      "torch.Size([1, 28, 28, 3])\n",
      "--------------------\n",
      "4\n",
      "torch.Size([0, 28, 28, 3])\n",
      "tensor([], size=(0, 28, 28, 3))\n",
      "--------------------\n",
      "3\n",
      "torch.Size([4, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 张量的分割    torch.chunk、torch.split、torch.unbind\n",
    "\n",
    "# 张量的分割即将张量拆分为多个张量，需要指定分割方案(传入数字或传入列表)\n",
    "# 传入一个List即指定按列表制定的方案分割\n",
    "\n",
    "a2 = torch.randn(4, 28, 28, 3)  # a是一个图像集，有4张图\n",
    "\n",
    "result = torch.chunk(a2, 4, dim=0)  # 指定分割为4份\n",
    "print(type(result))  # 返回一个元组\n",
    "print(len(result))\n",
    "print(result[0].shape)  # 第一份是一个1张图片的图像集\n",
    "print('-'*20)\n",
    "\n",
    "result = torch.split(a2, 1, dim=0)  # 注意！！这里第二个参数是指定每个部分有多少个元素！而不是指定分割成多少份！\n",
    "print(len(result))\n",
    "print(result[0].shape)  # 第一份是一个1张图片的图像集\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "a2 = torch.randn(4, 28, 28, 3)   # a是一个图像集，有4张图\n",
    "result = torch.split(a2, [1,2,1,0], dim=0)  # 指定分割方式，传入一个List\n",
    "print(len(result))  # 被分割成了4份\n",
    "print(result[-1].shape)  # 最后一份没有图片\n",
    "print(result[-1])  # 是一个空tensor，只有形状\n",
    "\n",
    "print('-'*20)\n",
    "# 如果想等份分割，每份都只有1个元素，可用 unbind 方法。\n",
    "# 注意：分完后，分割的那个维度消失。\n",
    "a2 = torch.randn(4, 28, 28, 3)\n",
    "result = torch.unbind(a2, dim=3)  # 把最后一个维度，即通道进行分割\n",
    "print(len(result))\n",
    "print(result[0].shape)  # 分完后，被分割的那个维度消失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的数据统计（tensorflow）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((3,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1范数： tf.Tensor(29.402458, shape=(), dtype=float32)\n",
      "L2范数： tf.Tensor(4.368295, shape=(), dtype=float32)\n",
      "--------------------\n",
      "最大值： tf.Tensor(\n",
      "[[0.6605253  0.38460138 0.69443697 0.6671217  0.99093324]\n",
      " [0.7861617  0.56994903 0.9839496  0.49521574 0.7138936 ]\n",
      " [0.13651751 0.85456795 0.813725   0.8225131  0.88694656]\n",
      " [0.76268965 0.7707564  0.7047183  0.9604446  0.8622231 ]], shape=(4, 5), dtype=float32)\n",
      "最大值所在索引： tf.Tensor(\n",
      "[[1 1 2 0 2]\n",
      " [2 0 1 0 2]\n",
      " [2 1 0 1 2]\n",
      " [0 2 2 0 1]], shape=(4, 5), dtype=int64)\n",
      "最小值： tf.Tensor(\n",
      "[[3.47301096e-01 4.56945767e-04 5.89605093e-01 5.90759754e-01\n",
      "  7.57497251e-01]\n",
      " [1.22475084e-02 3.21596980e-01 1.70186356e-01 4.39687520e-01\n",
      "  8.02342892e-02]\n",
      " [1.14157625e-01 2.41898000e-01 4.56167698e-01 1.14263453e-01\n",
      "  1.98546633e-01]\n",
      " [4.91749868e-02 2.01254293e-01 1.93469286e-01 2.29979366e-01\n",
      "  1.00287892e-01]], shape=(4, 5), dtype=float32)\n",
      "最小值所在索引： tf.Tensor(\n",
      "[[0 2 1 2 0]\n",
      " [1 1 2 2 0]\n",
      " [1 2 1 2 0]\n",
      " [2 0 1 2 0]], shape=(4, 5), dtype=int64)\n",
      "(可指定维度)求均值： tf.Tensor(0.49004096, shape=(), dtype=float32)\n",
      "(可指定维度)求和： tf.Tensor(29.402458, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 1、范数\n",
    "b = tf.constant(data, dtype=tf.float32)\n",
    "\n",
    "l1_b = tf.norm(b, 1)\n",
    "print('L1范数：', l1_b)\n",
    "\n",
    "l2_b = tf.norm(b, 2)\n",
    "print('L2范数：', l2_b)\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "# 2、维度最大值、最小值、均值、和\n",
    "print('最大值：', tf.reduce_max(b, axis=0))  # 不指定axis，返回张量中最大的那一个数值\n",
    "print('最大值所在索引：', tf.argmax(b))  # 默认axis=0，返回该维度上最大值所在位置（索引）\n",
    "print('最小值：', tf.reduce_min(b, axis=0))\n",
    "print('最小值所在索引：', tf.argmin(b))\n",
    "print('(可指定维度)求均值：', tf.reduce_mean(b))  # 不指定axis则求所有数的均值\n",
    "print('(可指定维度)求和：', tf.reduce_sum(b))    # 不指定axis则求所有数的和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的数据统计（pytorch）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1范数： tensor(29.40245438)\n",
      "L2范数： tensor(4.36829519)\n",
      "--------------------\n",
      "最大值和索引： torch.return_types.max(\n",
      "values=tensor([[0.66052532, 0.38460138, 0.69443697, 0.66712171, 0.99093324],\n",
      "        [0.78616172, 0.56994903, 0.98394960, 0.49521574, 0.71389359],\n",
      "        [0.13651751, 0.85456795, 0.81372499, 0.82251310, 0.88694656],\n",
      "        [0.76268965, 0.77075642, 0.70471829, 0.96044463, 0.86222309]]),\n",
      "indices=tensor([[1, 1, 2, 0, 2],\n",
      "        [2, 0, 1, 0, 2],\n",
      "        [2, 1, 0, 1, 2],\n",
      "        [0, 2, 2, 0, 1]]))\n",
      "最小值和索引： torch.return_types.min(\n",
      "values=tensor([[3.47301096e-01, 4.56945767e-04, 5.89605093e-01, 5.90759754e-01,\n",
      "         7.57497251e-01],\n",
      "        [1.22475084e-02, 3.21596980e-01, 1.70186356e-01, 4.39687520e-01,\n",
      "         8.02342892e-02],\n",
      "        [1.14157625e-01, 2.41898000e-01, 4.56167698e-01, 1.14263453e-01,\n",
      "         1.98546633e-01],\n",
      "        [4.91749868e-02, 2.01254293e-01, 1.93469286e-01, 2.29979366e-01,\n",
      "         1.00287892e-01]]),\n",
      "indices=tensor([[0, 2, 1, 2, 0],\n",
      "        [1, 1, 2, 2, 0],\n",
      "        [1, 2, 1, 2, 0],\n",
      "        [2, 0, 1, 2, 0]]))\n",
      "可以单独求最大值所在索引（最小值所在索引同理）： tensor([[1, 1, 2, 0, 2],\n",
      "        [2, 0, 1, 0, 2],\n",
      "        [2, 1, 0, 1, 2],\n",
      "        [0, 2, 2, 0, 1]])\n",
      "(可指定维度)求均值： tensor(0.49004093)\n",
      "(可指定维度)求和： tensor(29.40245628)\n"
     ]
    }
   ],
   "source": [
    "# 1、范数\n",
    "a = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "l1_a = torch.norm(a, 1)\n",
    "print('L1范数：', l1_a)\n",
    "\n",
    "l2_a = torch.norm(a, 2)\n",
    "print('L2范数：', l2_a)\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "# 2、维度最大值、最小值、均值、和\n",
    "print('最大值和索引：', torch.max(a, dim=0))  # 不指定dim，会返回张量中最大的那一个数值\n",
    "# 指定好维度dim后，可以看到max方法返回两个元素，一个是最大值矩阵，一个是最大值所在索引\n",
    "\n",
    "print('最小值和索引：', torch.min(a, dim=0))  # 不指定dim，会返回张量中最小的那一个数值\n",
    "# 指定好维度dim后，可以看到max方法返回两个元素，一个是最小值矩阵，一个是最小值所在索引\n",
    "\n",
    "print('可以单独求最大值所在索引（最小值所在索引同理）：', torch.argmax(a, dim=0))\n",
    "\n",
    "print('(可指定维度)求均值：', torch.mean(a))  # 不指定dim则求所有数的均值\n",
    "print('(可指定维度)求和：', torch.sum(a))    # 不指定dim则求所有数的和\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58edbf06b447562b2f859b4d256e1fe9b5ec2dc8d59984181fb6f62f1d7242b1"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
